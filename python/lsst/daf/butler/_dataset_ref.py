# This file is part of daf_butler.
#
# Developed for the LSST Data Management System.
# This product includes software developed by the LSST Project
# (http://www.lsst.org).
# See the COPYRIGHT file at the top-level directory of this distribution
# for details of code ownership.
#
# This software is dual licensed under the GNU General Public License and also
# under a 3-clause BSD license. Recipients may choose which of these licenses
# to use; please see the files gpl-3.0.txt and/or bsd_license.txt,
# respectively.  If you choose the GPL option then the following text applies
# (but note that there is still no warranty even if you opt for BSD instead):
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import annotations

__all__ = [
    "AmbiguousDatasetError",
    "DatasetDatastoreRecords",
    "DatasetId",
    "DatasetIdFactory",
    "DatasetIdGenEnum",
    "DatasetRef",
    "SerializedDatasetRef",
    "SerializedDatasetRefContainerV1",
    "SerializedDatasetRefContainers",
]

import enum
import logging
import sys
import uuid
from collections.abc import Callable, Iterable, Mapping
from typing import (
    TYPE_CHECKING,
    Annotated,
    Any,
    ClassVar,
    Literal,
    Protocol,
    Self,
    TypeAlias,
    cast,
    runtime_checkable,
)

import pydantic
from pydantic import StrictStr

from lsst.utils.classes import immutable

from ._config_support import LookupKey
from ._dataset_type import DatasetType, SerializedDatasetType
from ._named import NamedKeyDict
from ._uuid import generate_uuidv7
from .datastore.stored_file_info import StoredDatastoreItemInfo
from .dimensions import (
    DataCoordinate,
    DimensionDataAttacher,
    DimensionDataExtractor,
    DimensionGroup,
    DimensionUniverse,
    SerializableDimensionData,
    SerializedDataCoordinate,
    SerializedDataId,
)
from .json import from_json_pydantic, to_json_pydantic
from .persistence_context import PersistenceContextVars

if TYPE_CHECKING:
    from ._storage_class import StorageClass
    from .registry import Registry

# Per-dataset records grouped by opaque table name, usually there is just one
# opaque table.
DatasetDatastoreRecords: TypeAlias = Mapping[str, list[StoredDatastoreItemInfo]]


_LOG = logging.getLogger(__name__)


class AmbiguousDatasetError(Exception):
    """Raised when a `DatasetRef` is not resolved but should be.

    This happens when the `DatasetRef` has no ID or run but the requested
    operation requires one of them.
    """


@runtime_checkable
class _DatasetRefGroupedIterable(Protocol):
    """A package-private interface for iterables of `DatasetRef` that know how
    to efficiently group their contents by `DatasetType`.

    """

    def _iter_by_dataset_type(self) -> Iterable[tuple[DatasetType, Iterable[DatasetRef]]]:
        """Iterate over `DatasetRef` instances, one `DatasetType` at a time.

        Returns
        -------
        grouped : `~collections.abc.Iterable` [ `tuple` [ `DatasetType`, \
                `~collections.abc.Iterable` [ `DatasetRef` ]
            An iterable of tuples, in which the first element is a dataset type
            and the second is an iterable of `DatasetRef` objects with exactly
            that dataset type.
        """
        ...


class DatasetIdGenEnum(enum.Enum):
    """Enum used to specify dataset ID generation options."""

    UNIQUE = 0
    """Unique mode generates unique ID for each inserted dataset, e.g.
    auto-generated by database or random UUID.
    """

    DATAID_TYPE = 1
    """In this mode ID is computed deterministically from a combination of
    dataset type and dataId.
    """

    DATAID_TYPE_RUN = 2
    """In this mode ID is computed deterministically from a combination of
    dataset type, dataId, and run collection name.
    """


class DatasetIdFactory:
    """Factory for dataset IDs (UUIDs).

    For now the logic is hard-coded and is controlled by the user-provided
    value of `DatasetIdGenEnum`. In the future we may implement a configurable
    logic that can guess `DatasetIdGenEnum` value from other parameters.
    """

    NS_UUID = uuid.UUID("840b31d9-05cd-5161-b2c8-00d32b280d0f")
    """Namespace UUID used for UUID5 generation. Do not change. This was
    produced by `uuid.uuid5(uuid.NAMESPACE_DNS, "lsst.org")`.
    """

    def makeDatasetId(
        self,
        run: str,
        datasetType: DatasetType,
        dataId: DataCoordinate,
        idGenerationMode: DatasetIdGenEnum,
    ) -> uuid.UUID:
        """Generate dataset ID for a dataset.

        Parameters
        ----------
        run : `str`
            Name of the RUN collection for the dataset.
        datasetType : `DatasetType`
            Dataset type.
        dataId : `DataCoordinate`
            Expanded data ID for the dataset.
        idGenerationMode : `DatasetIdGenEnum`
            ID generation option. `~DatasetIdGenEnum.UNIQUE` makes a random
            UUID4-type ID. `~DatasetIdGenEnum.DATAID_TYPE` makes a
            deterministic UUID5-type ID based on a dataset type name and
            ``dataId``.  `~DatasetIdGenEnum.DATAID_TYPE_RUN` makes a
            deterministic UUID5-type ID based on a dataset type name, run
            collection name, and ``dataId``.

        Returns
        -------
        datasetId : `uuid.UUID`
            Dataset identifier.
        """
        if idGenerationMode is DatasetIdGenEnum.UNIQUE:
            # Earlier versions of this code used UUIDv4. However, totally
            # random IDs create problems for Postgres insert performance,
            # because it scatters index updates randomly around the disk.
            # UUIDv7 has similar uniqueness properties to v4, but IDs generated
            # at the same time are close together in the index.
            return generate_uuidv7()
        else:
            # WARNING: If you modify this code make sure that the order of
            # items in the `items` list below never changes.
            items: list[tuple[str, str]] = []
            if idGenerationMode is DatasetIdGenEnum.DATAID_TYPE:
                items = [
                    ("dataset_type", datasetType.name),
                ]
            elif idGenerationMode is DatasetIdGenEnum.DATAID_TYPE_RUN:
                items = [
                    ("dataset_type", datasetType.name),
                    ("run", run),
                ]
            else:
                raise ValueError(f"Unexpected ID generation mode: {idGenerationMode}")

            for name, value in sorted(dataId.required.items()):
                items.append((name, str(value)))
            data = ",".join(f"{key}={value}" for key, value in items)
            return uuid.uuid5(self.NS_UUID, data)


# This is constant, so don't recreate a set for each instance
_serializedDatasetRefFieldsSet = {"id", "datasetType", "dataId", "run", "component"}


class SerializedDatasetRef(pydantic.BaseModel):
    """Simplified model of a `DatasetRef` suitable for serialization."""

    id: uuid.UUID
    datasetType: SerializedDatasetType | None = None
    dataId: SerializedDataCoordinate | None = None
    run: StrictStr | None = None
    component: StrictStr | None = None

    # Can not use "after" validator since in some cases the validator
    # seems to trigger with the datasetType field not yet set.
    @pydantic.model_validator(mode="before")  # type: ignore[attr-defined]
    @classmethod
    def check_consistent_parameters(cls, data: dict[str, Any]) -> dict[str, Any]:
        has_datasetType = data.get("datasetType") is not None
        has_dataId = data.get("dataId") is not None
        if has_datasetType is not has_dataId:
            raise ValueError("If specifying datasetType or dataId, must specify both.")

        if data.get("component") is not None and has_datasetType:
            raise ValueError("datasetType can not be set if component is given.")
        return data

    @classmethod
    def direct(
        cls,
        *,
        id: str,
        run: str,
        datasetType: dict[str, Any] | None = None,
        dataId: dict[str, Any] | None = None,
        component: str | None = None,
    ) -> SerializedDatasetRef:
        """Construct a `SerializedDatasetRef` directly without validators.

        Parameters
        ----------
        id : `str`
            The UUID in string form.
        run : `str`
            The run for this dataset.
        datasetType : `dict` [`str`, `typing.Any`]
            A representation of the dataset type.
        dataId : `dict` [`str`, `typing.Any`]
            A representation of the data ID.
        component : `str` or `None`
            Any component associated with this ref.

        Returns
        -------
        serialized : `SerializedDatasetRef`
            A Pydantic model representing the given parameters.

        Notes
        -----
        This differs from the pydantic "construct" method in that the arguments
        are explicitly what the model requires, and it will recurse through
        members, constructing them from their corresponding `direct` methods.

        The ``id`` parameter is a string representation of dataset ID, it is
        converted to UUID by this method.

        This method should only be called when the inputs are trusted.
        """
        serialized_datasetType = (
            SerializedDatasetType.direct(**datasetType) if datasetType is not None else None
        )
        serialized_dataId = SerializedDataCoordinate.direct(**dataId) if dataId is not None else None

        node = cls.model_construct(
            _fields_set=_serializedDatasetRefFieldsSet,
            id=uuid.UUID(id),
            datasetType=serialized_datasetType,
            dataId=serialized_dataId,
            run=sys.intern(run),
            component=component,
        )

        return node


DatasetId: TypeAlias = uuid.UUID
"""A type-annotation alias for dataset ID providing typing flexibility.
"""


@immutable
class DatasetRef:
    """Reference to a Dataset in a `Registry`.

    A `DatasetRef` may point to a Dataset that currently does not yet exist
    (e.g., because it is a predicted input for provenance).

    Parameters
    ----------
    datasetType : `DatasetType`
        The `DatasetType` for this Dataset.
    dataId : `DataCoordinate`
        A mapping of dimensions that labels the Dataset within a Collection.
    run : `str`
        The name of the run this dataset was associated with when it was
        created.
    id : `DatasetId`, optional
        The unique identifier assigned when the dataset is created. If ``id``
        is not specified, a new unique ID will be created.
    conform : `bool`, optional
        If `True` (default), call `DataCoordinate.standardize` to ensure that
        the data ID's dimensions are consistent with the dataset type's.
        `DatasetRef` instances for which those dimensions are not equal should
        not be created in new code, but are still supported for backwards
        compatibility.  New code should only pass `False` if it can guarantee
        that the dimensions are already consistent.
    id_generation_mode : `DatasetIdGenEnum`
        ID generation option. `~DatasetIdGenEnum.UNIQUE` makes a random
        UUID4-type ID. `~DatasetIdGenEnum.DATAID_TYPE` makes a
        deterministic UUID5-type ID based on a dataset type name and
        ``dataId``.  `~DatasetIdGenEnum.DATAID_TYPE_RUN` makes a
        deterministic UUID5-type ID based on a dataset type name, run
        collection name, and ``dataId``.
    datastore_records : `DatasetDatastoreRecords` or `None`
        Datastore records to attach.

    Notes
    -----
    See also :ref:`daf_butler_organizing_datasets`
    """

    _serializedType: ClassVar[type[pydantic.BaseModel]] = SerializedDatasetRef
    __slots__ = (
        "_id",
        "datasetType",
        "dataId",
        "run",
        "_datastore_records",
    )

    def __init__(
        self,
        datasetType: DatasetType,
        dataId: DataCoordinate,
        run: str,
        *,
        id: DatasetId | None = None,
        conform: bool = True,
        id_generation_mode: DatasetIdGenEnum = DatasetIdGenEnum.UNIQUE,
        datastore_records: DatasetDatastoreRecords | None = None,
    ):
        self.datasetType = datasetType
        if conform:
            self.dataId = DataCoordinate.standardize(dataId, dimensions=datasetType.dimensions)
        else:
            self.dataId = dataId
        self.run = run
        if id is not None:
            self._id = id.int
        else:
            self._id = (
                DatasetIdFactory()
                .makeDatasetId(self.run, self.datasetType, self.dataId, id_generation_mode)
                .int
            )
        self._datastore_records = datastore_records

    @property
    def id(self) -> DatasetId:
        """Primary key of the dataset (`DatasetId`).

        Cannot be changed after a `DatasetRef` is constructed.
        """
        return uuid.UUID(int=self._id)

    def __eq__(self, other: Any) -> bool:
        try:
            return (self.datasetType, self.dataId, self.id) == (other.datasetType, other.dataId, other.id)
        except AttributeError:
            return NotImplemented

    def __hash__(self) -> int:
        return hash((self.datasetType, self.dataId, self.id))

    @property
    def dimensions(self) -> DimensionGroup:
        """Dimensions associated with the underlying `DatasetType`."""
        return self.datasetType.dimensions

    def __repr__(self) -> str:
        # We delegate to __str__ (i.e use "!s") for the data ID) below because
        # DataCoordinate's __repr__ - while adhering to the guidelines for
        # __repr__ - is much harder to users to read, while its __str__ just
        # produces a dict that can also be passed to DatasetRef's constructor.
        return f"DatasetRef({self.datasetType!r}, {self.dataId!s}, run={self.run!r}, id={self.id})"

    def __str__(self) -> str:
        s = (
            f"{self.datasetType.name}@{self.dataId!s} [sc={self.datasetType.storageClass_name}]"
            f" (run={self.run} id={self.id})"
        )
        return s

    def __lt__(self, other: Any) -> bool:
        # Sort by run, DatasetType name and then by DataCoordinate
        # The __str__ representation is probably close enough but we
        # need to ensure that sorting a DatasetRef matches what you would
        # get if you sorted DatasetType+DataCoordinate
        if not isinstance(other, type(self)):
            return NotImplemented

        # Group by run if defined, takes precedence over DatasetType
        self_run = "" if self.run is None else self.run
        other_run = "" if other.run is None else other.run

        # Compare tuples in the priority order
        return (self_run, self.datasetType, self.dataId) < (other_run, other.datasetType, other.dataId)

    def to_simple(self, minimal: bool = False) -> SerializedDatasetRef:
        """Convert this class to a simple python type.

        This makes it suitable for serialization.

        Parameters
        ----------
        minimal : `bool`, optional
            Use minimal serialization. Requires Registry to convert
            back to a full type.

        Returns
        -------
        simple : `dict` or `int`
            The object converted to a dictionary.
        """
        if minimal:
            # The only thing needed to uniquely define a DatasetRef is its id
            # so that can be used directly if it is not a component DatasetRef.
            # Store is in a dict to allow us to easily add the planned origin
            # information later without having to support an int and dict in
            # simple form.
            simple: dict[str, Any] = {"id": self.id}
            if self.isComponent():
                # We can still be a little minimalist with a component
                # but we will also need to record the datasetType component
                simple["component"] = self.datasetType.component()
            return SerializedDatasetRef(**simple)

        return SerializedDatasetRef(
            datasetType=self.datasetType.to_simple(minimal=minimal),
            dataId=self.dataId.to_simple(),
            run=self.run,
            id=self.id,
        )

    @classmethod
    def from_simple(
        cls,
        simple: SerializedDatasetRef,
        universe: DimensionUniverse | None = None,
        registry: Registry | None = None,
        datasetType: DatasetType | None = None,
    ) -> DatasetRef:
        """Construct a new object from simplified form.

        Generally this is data returned from the `to_simple` method.

        Parameters
        ----------
        simple : `dict` of [`str`, `Any`]
            The value returned by `to_simple()`.
        universe : `DimensionUniverse`
            The special graph of all known dimensions.
            Can be `None` if a registry is provided.
        registry : `lsst.daf.butler.Registry`, optional
            Registry to use to convert simple form of a DatasetRef to
            a full `DatasetRef`. Can be `None` if a full description of
            the type is provided along with a universe.
        datasetType : DatasetType, optional
            If datasetType is supplied, this will be used as the datasetType
            object in the resulting DatasetRef instead of being read from
            the `SerializedDatasetRef`. This is useful when many refs share
            the same type as memory can be saved. Defaults to None.

        Returns
        -------
        ref : `DatasetRef`
            Newly-constructed object.
        """
        cache = PersistenceContextVars.datasetRefs.get()
        key = simple.id.int
        if cache is not None and (ref := cache.get(key, None)) is not None:
            if datasetType is not None:
                if (component := datasetType.component()) is not None:
                    ref = ref.makeComponentRef(component)
                ref = ref.overrideStorageClass(datasetType.storageClass_name)
                return ref
            if simple.datasetType is not None:
                _, component = DatasetType.splitDatasetTypeName(simple.datasetType.name)
                if component is not None:
                    ref = ref.makeComponentRef(component)
                if simple.datasetType.storageClass is not None:
                    ref = ref.overrideStorageClass(simple.datasetType.storageClass)
                    return ref
            # If dataset type is not given ignore the cache, because we can't
            # reliably return the right storage class.
        # Minimalist component will just specify component and id and
        # require registry to reconstruct
        if simple.datasetType is None and simple.dataId is None and simple.run is None:
            if registry is None:
                raise ValueError("Registry is required to construct component DatasetRef from integer id")
            if simple.id is None:
                raise ValueError("For minimal DatasetRef the ID must be defined.")
            ref = registry.getDataset(simple.id)
            if ref is None:
                raise RuntimeError(f"No matching dataset found in registry for id {simple.id}")
            if simple.component:
                ref = ref.makeComponentRef(simple.component)
        else:
            if universe is None:
                if registry is None:
                    raise ValueError("One of universe or registry must be provided.")
                universe = registry.dimensions
            if datasetType is None:
                if simple.datasetType is None:
                    raise ValueError("Cannot determine Dataset type of this serialized class")
                datasetType = DatasetType.from_simple(
                    simple.datasetType, universe=universe, registry=registry
                )
            if simple.dataId is None:
                # mypy
                raise ValueError("The DataId must be specified to construct a DatasetRef")
            dataId = DataCoordinate.from_simple(simple.dataId, universe=universe)
            # Check that simple ref is resolved.
            if simple.run is None:
                dstr = ""
                if simple.datasetType is None:
                    dstr = f" (datasetType={datasetType.name!r})"
                raise ValueError(
                    "Run collection name is missing from serialized representation. "
                    f"Encountered with {simple!r}{dstr}."
                )
            ref = cls(
                datasetType,
                dataId,
                id=simple.id,
                run=simple.run,
            )
        if cache is not None:
            if ref.datasetType.component() is not None:
                cache[key] = ref.makeCompositeRef()
            else:
                cache[key] = ref
        return ref

    to_json = to_json_pydantic
    from_json: ClassVar[Callable[..., Self]] = cast(Callable[..., Self], classmethod(from_json_pydantic))

    @classmethod
    def _unpickle(
        cls,
        datasetType: DatasetType,
        dataId: DataCoordinate,
        id: DatasetId,
        run: str,
        datastore_records: DatasetDatastoreRecords | None,
    ) -> DatasetRef:
        """Create new `DatasetRef`.

        A custom factory method for use by `__reduce__` as a workaround for
        its lack of support for keyword arguments.
        """
        return cls(datasetType, dataId, id=id, run=run, datastore_records=datastore_records)

    def __reduce__(self) -> tuple:
        return (
            self._unpickle,
            (self.datasetType, self.dataId, self.id, self.run, self._datastore_records),
        )

    def __deepcopy__(self, memo: dict) -> DatasetRef:
        # DatasetRef is recursively immutable; see note in @immutable
        # decorator.
        return self

    def expanded(self, dataId: DataCoordinate) -> DatasetRef:
        """Return a new `DatasetRef` with the given expanded data ID.

        Parameters
        ----------
        dataId : `DataCoordinate`
            Data ID for the new `DatasetRef`.  Must compare equal to the
            original data ID.

        Returns
        -------
        ref : `DatasetRef`
            A new `DatasetRef` with the given data ID.
        """
        assert dataId == self.dataId
        return DatasetRef(
            datasetType=self.datasetType,
            dataId=dataId,
            id=self.id,
            run=self.run,
            conform=False,
            datastore_records=self._datastore_records,
        )

    def isComponent(self) -> bool:
        """Indicate whether this `DatasetRef` refers to a component.

        Returns
        -------
        isComponent : `bool`
            `True` if this `DatasetRef` is a component, `False` otherwise.
        """
        return self.datasetType.isComponent()

    def isComposite(self) -> bool:
        """Boolean indicating whether this `DatasetRef` is a composite type.

        Returns
        -------
        isComposite : `bool`
            `True` if this `DatasetRef` is a composite type, `False`
            otherwise.
        """
        return self.datasetType.isComposite()

    def _lookupNames(self) -> tuple[LookupKey, ...]:
        """Name keys to use when looking up this DatasetRef in a configuration.

        The names are returned in order of priority.

        Returns
        -------
        names : `tuple` of `LookupKey`
            Tuple of the `DatasetType` name and the `StorageClass` name.
            If ``instrument`` is defined in the dataId, each of those names
            is added to the start of the tuple with a key derived from the
            value of ``instrument``.
        """
        # Special case the instrument Dimension since we allow configs
        # to include the instrument name in the hierarchy.
        names: tuple[LookupKey, ...] = self.datasetType._lookupNames()

        if "instrument" in self.dataId:
            names = tuple(n.clone(dataId={"instrument": self.dataId["instrument"]}) for n in names) + names

        return names

    @staticmethod
    def groupByType(refs: Iterable[DatasetRef]) -> NamedKeyDict[DatasetType, list[DatasetRef]]:
        """Group an iterable of `DatasetRef` by `DatasetType`.

        Parameters
        ----------
        refs : `~collections.abc.Iterable` [ `DatasetRef` ]
            `DatasetRef` instances to group.

        Returns
        -------
        grouped : `NamedKeyDict` [ `DatasetType`, `list` [ `DatasetRef` ] ]
            Grouped `DatasetRef` instances.

        Notes
        -----
        When lazy item-iterables are acceptable instead of a full mapping,
        `iter_by_type` can in some cases be far more efficient.
        """
        result: NamedKeyDict[DatasetType, list[DatasetRef]] = NamedKeyDict()
        for ref in refs:
            result.setdefault(ref.datasetType, []).append(ref)
        return result

    @staticmethod
    def iter_by_type(
        refs: Iterable[DatasetRef],
    ) -> Iterable[tuple[DatasetType, Iterable[DatasetRef]]]:
        """Group an iterable of `DatasetRef` by `DatasetType` with special
        hooks for custom iterables that can do this efficiently.

        Parameters
        ----------
        refs : `~collections.abc.Iterable` [ `DatasetRef` ]
            `DatasetRef` instances to group.  If this satisfies the
            `_DatasetRefGroupedIterable` protocol, its
            `~_DatasetRefGroupedIterable._iter_by_dataset_type` method will
            be called.

        Returns
        -------
        grouped : `~collections.abc.Iterable` [ `tuple` [ `DatasetType`, \
                `~collections.abc.Iterable` [ `DatasetRef` ] ]]
            Grouped `DatasetRef` instances.
        """
        if isinstance(refs, _DatasetRefGroupedIterable):
            return refs._iter_by_dataset_type()
        return DatasetRef.groupByType(refs).items()

    def makeCompositeRef(self) -> DatasetRef:
        """Create a `DatasetRef` of the composite from a component ref.

        Requires that this `DatasetRef` is a component.

        Returns
        -------
        ref : `DatasetRef`
            A `DatasetRef` with a dataset type that corresponds to the
            composite parent of this component, and the same ID and run
            (which may be `None`, if they are `None` in ``self``).
        """
        # Assume that the data ID does not need to be standardized
        # and should match whatever this ref already has.
        return DatasetRef(
            self.datasetType.makeCompositeDatasetType(),
            self.dataId,
            id=self.id,
            run=self.run,
            conform=False,
            datastore_records=self._datastore_records,
        )

    def makeComponentRef(self, name: str) -> DatasetRef:
        """Create a `DatasetRef` that corresponds to a component.

        Parameters
        ----------
        name : `str`
            Name of the component.

        Returns
        -------
        ref : `DatasetRef`
            A `DatasetRef` with a dataset type that corresponds to the given
            component, and the same ID and run
            (which may be `None`, if they are `None` in ``self``).
        """
        # Assume that the data ID does not need to be standardized
        # and should match whatever this ref already has.
        return DatasetRef(
            self.datasetType.makeComponentDatasetType(name),
            self.dataId,
            id=self.id,
            run=self.run,
            conform=False,
            datastore_records=self._datastore_records,
        )

    def overrideStorageClass(self, storageClass: str | StorageClass) -> DatasetRef:
        """Create a new `DatasetRef` from this one, but with a modified
        `DatasetType` that has a different `StorageClass`.

        Parameters
        ----------
        storageClass : `str` or `StorageClass`
            The new storage class.

        Returns
        -------
        modified : `DatasetRef`
            A new dataset reference that is the same as the current one but
            with a different storage class in the `DatasetType`.
        """
        return self.replace(storage_class=storageClass)

    def replace(
        self,
        *,
        id: DatasetId | None = None,
        run: str | None = None,
        storage_class: str | StorageClass | None = None,
        datastore_records: DatasetDatastoreRecords | None | Literal[False] = False,
    ) -> DatasetRef:
        """Create a new `DatasetRef` from this one, but with some modified
        attributes.

        Parameters
        ----------
        id : `DatasetId` or `None`
            If not `None` then update dataset ID.
        run : `str` or `None`
            If not `None` then update run collection name. If ``dataset_id`` is
            `None` then this will also cause new dataset ID to be generated.
        storage_class : `str` or `StorageClass` or `None`
            The new storage class. If not `None`, replaces existing storage
            class.
        datastore_records : `DatasetDatastoreRecords` or `None`
            New datastore records. If `None` remove all records. By default
            datastore records are preserved.

        Returns
        -------
        modified : `DatasetRef`
            A new dataset reference with updated attributes.
        """
        if datastore_records is False:
            datastore_records = self._datastore_records
        if storage_class is None:
            datasetType = self.datasetType
        else:
            datasetType = self.datasetType.overrideStorageClass(storage_class)
        if run is None:
            run = self.run
            # Do not regenerate dataset ID if run is the same.
            if id is None:
                id = self.id
        return DatasetRef(
            datasetType=datasetType,
            dataId=self.dataId,
            run=run,
            id=id,
            conform=False,
            datastore_records=datastore_records,
        )

    def is_compatible_with(self, other: DatasetRef) -> bool:
        """Determine if the given `DatasetRef` is compatible with this one.

        Parameters
        ----------
        other : `DatasetRef`
            Dataset ref to check.

        Returns
        -------
        is_compatible : `bool`
            Returns `True` if the other dataset ref is either the same as this
            or the dataset type associated with the other is compatible with
            this one and the dataId and dataset ID match.

        Notes
        -----
        Compatibility requires that the dataId and dataset ID match and the
        `DatasetType` is compatible. Compatibility is defined as the storage
        class associated with the dataset type of the other ref can be
        converted to this storage class.

        Specifically this means that if you have done:

        .. code-block:: py

            new_ref = ref.overrideStorageClass(sc)

        and this is successful, then the guarantee is that:

        .. code-block:: py

            assert ref.is_compatible_with(new_ref) is True

        since we know that the python type associated with the new ref can
        be converted to the original python type. The reverse is not guaranteed
        and depends on whether bidirectional converters have been registered.
        """
        if self.id != other.id:
            return False
        if self.dataId != other.dataId:
            return False
        if self.run != other.run:
            return False
        return self.datasetType.is_compatible_with(other.datasetType)

    datasetType: DatasetType
    """The definition of this dataset (`DatasetType`).

    Cannot be changed after a `DatasetRef` is constructed.
    """

    dataId: DataCoordinate
    """A mapping of `Dimension` primary key values that labels the dataset
    within a Collection (`DataCoordinate`).

    Cannot be changed after a `DatasetRef` is constructed.
    """

    run: str
    """The name of the run that produced the dataset.

    Cannot be changed after a `DatasetRef` is constructed.
    """

    datastore_records: DatasetDatastoreRecords | None
    """Optional datastore records (`DatasetDatastoreRecords`).

    Cannot be changed after a `DatasetRef` is constructed.
    """


class MinimalistSerializableDatasetRef(pydantic.BaseModel):
    """Minimal information needed to define a DatasetRef.

    The ID is not included and is presumed to be the key to a mapping
    to this information.
    """

    model_config = pydantic.ConfigDict(frozen=True)

    dataset_type_name: str
    """Name of the dataset type."""

    run: str
    """Name of the RUN collection."""

    data_id: SerializedDataId
    """Data coordinate of this dataset."""

    def to_dataset_ref(
        self,
        id: DatasetId,
        *,
        dataset_type: DatasetType,
        universe: DimensionUniverse,
        attacher: DimensionDataAttacher | None = None,
    ) -> DatasetRef:
        """Convert serialized object to a `DatasetRef`.

        Parameters
        ----------
        id : `DatasetId`
            UUID identifying the dataset.
        dataset_type : `DatasetType`
            `DatasetType` record corresponding to the dataset type name in the
            serialized object.
        universe : `DimensionUniverse`
            Dimension universe for the dataset.
        attacher : `DimensionDataAttacher`, optional
            If provided, will be used to add dimension records to the
            deserialized `DatasetRef` instance.

        Returns
        -------
        ref : `DatasetRef`
            The deserialized object.
        """
        assert dataset_type.name == self.dataset_type_name, (
            "Given DatasetType does not match the serialized dataset type name"
        )
        simple_data_id = SerializedDataCoordinate(dataId=self.data_id)
        data_id = DataCoordinate.from_simple(simple=simple_data_id, universe=universe)
        if attacher:
            data_ids = attacher.attach(dataset_type.dimensions, [data_id])
            data_id = data_ids[0]
        return DatasetRef(
            id=id,
            run=self.run,
            datasetType=dataset_type,
            dataId=data_id,
        )

    @staticmethod
    def from_dataset_ref(ref: DatasetRef) -> MinimalistSerializableDatasetRef:
        """Serialize a ``DatasetRef` to a simplified format.

        Parameters
        ----------
        ref : `DatasetRef`
            `DatasetRef` object to serialize.
        """
        return MinimalistSerializableDatasetRef(
            dataset_type_name=ref.datasetType.name, run=ref.run, data_id=dict(ref.dataId.mapping)
        )


class SerializedDatasetRefContainer(pydantic.BaseModel):
    """Serializable model for a collection of DatasetRef.

    Dimension records are not included.
    """

    model_config = pydantic.ConfigDict(extra="allow", frozen=True)
    container_version: str


class SerializedDatasetRefContainerV1(SerializedDatasetRefContainer):
    """Serializable model for a collection of DatasetRef.

    Dimension records are not included.
    """

    container_version: Literal["V1"] = "V1"

    universe_version: int
    """Dimension universe version."""

    universe_namespace: str
    """Dimension universe namespace."""

    dataset_types: dict[str, SerializedDatasetType]
    """Dataset types indexed by their name."""

    compact_refs: dict[uuid.UUID, MinimalistSerializableDatasetRef]
    """Minimal dataset ref information indexed by UUID."""

    dimension_records: SerializableDimensionData | None = None
    """Dimension record information"""

    def __len__(self) -> int:
        """Return the number of datasets in the container."""
        return len(self.compact_refs)

    @classmethod
    def from_refs(cls, refs: Iterable[DatasetRef]) -> Self:
        """Construct a serializable form from a list of `DatasetRef`.

        Parameters
        ----------
        refs : `~collections.abc.Iterable` [ `DatasetRef` ]
            The datasets to include in the container.
        """
        # The serialized DatasetRef contains a lot of duplicated information.
        # We also want to drop dimension records and assume that the records
        # are already in the registry.
        universe: DimensionUniverse | None = None
        dataset_types: dict[str, SerializedDatasetType] = {}
        compact_refs: dict[uuid.UUID, MinimalistSerializableDatasetRef] = {}
        data_ids: list[DataCoordinate] = []
        dimensions: list[DimensionGroup] = []
        for ref in refs:
            if universe is None:
                universe = ref.datasetType.dimensions.universe
            if (name := ref.datasetType.name) not in dataset_types:
                dataset_types[name] = ref.datasetType.to_simple()
            compact_refs[ref.id] = MinimalistSerializableDatasetRef.from_dataset_ref(ref)
            if ref.dataId.hasRecords():
                dimensions.append(ref.datasetType.dimensions)
                data_ids.append(ref.dataId)

        # Extract dimension record metadata if present.
        dimension_records = None
        if data_ids and len(compact_refs) == len(data_ids):
            dimension_group = DimensionGroup.union(*dimensions, universe=universe)

            # Records were attached to all refs. Store them.
            extractor = DimensionDataExtractor.from_dimension_group(
                dimension_group,
                ignore_cached=False,
                include_skypix=False,
            )
            extractor.update(data_ids)
            dimension_records = SerializableDimensionData.from_record_sets(extractor.records.values())

        if universe:
            universe_version = universe.version
            universe_namespace = universe.namespace
        else:
            # No refs so no universe.
            universe_version = 0
            universe_namespace = "unknown"
        return cls(
            universe_version=universe_version,
            universe_namespace=universe_namespace,
            dataset_types=dataset_types,
            compact_refs=compact_refs,
            dimension_records=dimension_records,
        )

    def to_refs(self, universe: DimensionUniverse) -> list[DatasetRef]:
        """Construct the original `DatasetRef`.

        Parameters
        ----------
        universe : `DimensionUniverse`
            The universe to use when constructing the `DatasetRef`.

        Returns
        -------
        refs : `list` [ `DatasetRef` ]
            The `DatasetRef` that were serialized.
        """
        if not self.compact_refs:
            return []

        if universe.namespace != self.universe_namespace:
            raise RuntimeError(
                f"Can not convert to refs in universe {universe.namespace} that were created from "
                f"universe {self.universe_namespace}"
            )

        if universe.version != self.universe_version:
            _LOG.warning(
                "Universe mismatch when attempting to reconstruct DatasetRef from serialized form. "
                "Serialized with version %d but asked to use version %d.",
                self.universe_version,
                universe.version,
            )

        # Reconstruct the DatasetType objects.
        dataset_types = {
            name: DatasetType.from_simple(dtype, universe=universe)
            for name, dtype in self.dataset_types.items()
        }

        # Dimension records can be attached if available.
        # We assume that all dimension information was stored.
        attacher = None
        if self.dimension_records:
            attacher = DimensionDataAttacher(
                deserializers=self.dimension_records.make_deserializers(universe)
            )

        refs: list[DatasetRef] = []
        for id_, minimal in self.compact_refs.items():
            ref = minimal.to_dataset_ref(
                id_,
                dataset_type=dataset_types[minimal.dataset_type_name],
                universe=universe,
                attacher=attacher,
            )
            refs.append(ref)
        return refs


SerializedDatasetRefContainers: TypeAlias = Annotated[
    SerializedDatasetRefContainerV1,
    pydantic.Field(discriminator="container_version"),
]
